\section{Introduction}

Speech is the most natural way of expressing ourselves as humans. It is only natural then to extend this communication medium to computer applications. Speech emotion recognition (SER) systems are collections of methodologies that process and classify speech signals to detect embedded emotions. These systems have numerous applications in human-computer interaction, customer service analysis, mental health monitoring, and entertainment.

SER presents unique challenges because emotional expressions in speech vary significantly across individuals, cultures, and contexts. The acoustic features that convey emotion can be subtle and often overlap with other speech characteristics. This assignment explores different approaches to SER, focusing on:

\begin{itemize}
    \item Extracting relevant features from speech signals using both time and frequency domain representations
    \item Developing and comparing 1D and 2D CNN architectures for emotion classification
    \item Analyzing the effects of different learning rates and activation functions on model performance
    \item Identifying confusing emotion classes and investigating the causes of misclassification
\end{itemize}

We utilize the CREMA dataset, which contains acted emotional speech recordings from multiple speakers expressing six basic emotions: sadness, anger, disgust, fear, happiness, and neutral. Our goal is to build effective SER models and provide insights into the relative performance of different feature extraction and classification techniques. 