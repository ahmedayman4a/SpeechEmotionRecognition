\section{Conclusion}

In this project, we implemented and evaluated several CNN architectures for speech emotion recognition using the CREMA dataset. We explored different approaches to feature extraction, model architecture, activation functions, and learning rates.

\subsection{Summary of Findings}

Our main findings can be summarized as follows:

\begin{enumerate}
    \item \textbf{2D CNN models excel}: The best performance was achieved by our 2D CNN model using SiLU activation function, reaching 64.1\% accuracy and 0.639 F1-score on the test set. This confirms the importance of spectral-temporal patterns in emotion recognition.
    
    \item \textbf{Activation functions matter}: Different architectures benefit from different activation functions. SiLU (Swish) was particularly effective for 2D CNN models, while ELU produced the best results for the 1D CNN. This highlights the importance of exploring beyond the standard ReLU activation.
    
    \item \textbf{Variable-length processing improves results}: Processing audio with its original temporal structure rather than fixed-size inputs improved performance (from 61.3\% to 62.2\% accuracy for the combined model), suggesting that preserving temporal dynamics is important for SER.
    
    \item \textbf{Learning rates require careful tuning}: Low learning rates (0.001) were optimal for 2D CNN and combined models, while moderate learning rates (0.01) worked better for the 1D CNN models.
    
    \item \textbf{Emotion recognition is inherently asymmetric}: Some emotions (Anger, Happiness) are significantly easier to recognize than others (Fear, Sadness), likely due to their more distinctive acoustic signatures.
\end{enumerate}

\subsection{Limitations}

Our work has several limitations:

\begin{itemize}
    \item \textbf{Dataset limitations}: The CREMA dataset, while diverse, consists of acted emotions rather than spontaneous emotional expressions, which may not fully represent real-world emotional speech.
    
    \item \textbf{No cross-dataset validation}: We trained and tested only on CREMA, so our models may not generalize well to other datasets or recording conditions.
    
    \item \textbf{Limited emotion set}: We focused on six basic emotions, but human emotional expression is much more nuanced and includes mixed and subtle emotions not captured in this study.
    
    \item \textbf{No linguistic content analysis}: We relied solely on acoustic features and did not incorporate linguistic content, which can provide important context for emotion interpretation.
\end{itemize}

\subsection{Future Work}

Based on our findings, several directions for future work appear promising:

\begin{enumerate}
    \item \textbf{Attention mechanisms}: Incorporating attention mechanisms could help models focus on the most emotionally salient parts of speech signals.
    
    \item \textbf{Transformer architectures}: Exploring transformer-based models, which have shown success in other audio processing tasks, could further improve performance.
    
    \item \textbf{Multi-modal approaches}: Combining acoustic features with visual cues (facial expressions) or linguistic content could provide complementary information for more accurate emotion recognition.
    
    \item \textbf{Data augmentation}: Developing effective data augmentation techniques specific to emotional speech could help address the limited size of existing datasets and improve generalization.
    
    \item \textbf{Custom activation functions}: Given the significant impact of activation functions on performance, designing or optimizing activation functions specifically for SER tasks could yield further improvements.

    \item \textbf{Fine-grained hyper-parameter tuning}: While we explored different learning rates and activation functions, many other hyper-parameters (e.g., batch siz e, optimizer, dropout rates) could be further optimized using systematic approaches like Bayesian optimization.
\end{enumerate}

In conclusion, our work demonstrates the effectiveness of CNN architectures for speech emotion recognition, particularly 2D CNN models with appropriate activation functions and input representations. The finding that 2D CNN with SiLU activation achieved the highest performance (64.1\%) highlights the importance of both model architecture and activation function selection in SER systems. These insights contribute to the ongoing development of more accurate and robust emotion recognition technologies.         